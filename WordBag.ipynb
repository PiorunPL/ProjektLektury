{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4a278fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "nlp = spacy.load(\"pl_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03ea3998",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ważna uwaga:\n",
    "Pliki z lekturami (trzeba funkcji ustawić argument is_fragment=False) do wczytania muszą mieć następującą strukturę:\n",
    "Autor\n",
    "\n",
    "Tytuł(dowolna ilość linii)\n",
    "\n",
    "Ewentualny numer ISBN\n",
    "\n",
    "Tekst(dowolna ilość linii)\n",
    "Ewentualna stopka po treście(przypis od wolnychlektur): -----\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Przy podawaniu fragmentu(is_fragment=True) skanowany jest cały plik, więc struktura tekstu jest dowolna\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "tworzy worek słów z pliku\n",
    "file_name - nazwa pliku z której utworzyc worek słów; with_stop_words - True/False mówiący czy usuwać stop wordsy czy nie\n",
    "is_fragment - True/False - mówi czy podawany plik to fragment do sprawdzenia, czy cały utwór\n",
    "\"\"\"\n",
    "def create_bag_of_words_from_file(file_name, is_fragment, with_stop_words=False, lemma=True, only_stop_words=False):\n",
    "#struktura worka: indeks to indeks, elementem jest lista ze słowem i liczebnością\n",
    "    bag = {}\n",
    "    with open(file_name,'r', encoding='utf-8') as file:\n",
    "#autor\n",
    "        if not is_fragment:\n",
    "            bag[0] = get_author(file)\n",
    "            \n",
    "        for line in file:\n",
    "            for word in line.split():\n",
    "#sprawdzanie czy linia nie jest stopką. Jak jest-skończ działanie\n",
    "                if not is_fragment and word == \"-----\":\n",
    "                    return bag\n",
    "            bag = create_bag_of_words_from_string(line, bag, with_stop_words, lemma, only_stop_words)\n",
    "    return bag\n",
    "\n",
    "def create_bag_of_words_from_string(string, bag, with_stop_words=False, lemma=True, only_stop_words=False):\n",
    "    #struktura worka: indeks to indeks, elementem jest lista ze słowem i liczebnością\n",
    "    stop_words = []\n",
    "#tablica znaków interpunkcyjnych do pozbycia się z wyrazów\n",
    "    punc = '''…!”„()-[]»{};:'\"\\,<>./?@#$%^&*_~«—'''+ \"\\n\" + \" \"\n",
    "#ewentualne przypisanie stopwordsów\n",
    "    if not with_stop_words or only_stop_words:\n",
    "        stop_words = get_stop_words_from_file('stopWords.txt')\n",
    "    words = words_from_line(string, lemma, punc)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "#jeśli jest stop wordsem- nie uwzględniamy (jeśli w funkcji było True, to 'stop_words' jest puste, czyli nie wejdzie do ifa)    \n",
    "        if word in stop_words:\n",
    "            if only_stop_words:\n",
    "                bag = add_to_bag(bag, word)\n",
    "            continue\n",
    "        elif not only_stop_words:\n",
    "            bag = add_to_bag(bag, word)\n",
    "    return bag\n",
    "\n",
    "def add_to_bag(bag, word):\n",
    "    if (word not in bag) and (len(word) != 0):\n",
    "        bag[word] = 1\n",
    "#inkrementacja liczności słowa, jeśli już istnieje w worku\n",
    "    elif len(word) != 0:\n",
    "        bag[word] += 1\n",
    "    return bag\n",
    "\n",
    "\"\"\"\n",
    "Tworzy tablicę słów w zależności od tego, czy używamy lematyzacji\n",
    "- Jeżeli nie używamy lematyzacji to usuwa ze słów ewentualne znaki interpunkcyjne\n",
    "- Jeżeli używamy lematyzacji to usuwa nadmierne spacje, generuje tokeny i zamienia je na słowa\n",
    "w wersji podstawowej zależnie od kontekstu w linijce. Pomija słowa będące znakami interpunkcyjnymi\n",
    "\"\"\"\n",
    "def words_from_line(line, lemma, punc):\n",
    "    words = []\n",
    "    if not lemma:\n",
    "        for word in line.split():\n",
    "            for ele in word:\n",
    "                if ele in punc:\n",
    "                    word = word.replace(ele, \"\")\n",
    "            words.append(word)\n",
    "    \n",
    "    else:\n",
    "        line = re.sub(' +', ' ', line)\n",
    "        tokens = nlp(line, disable=[\"parser\", \"ner\"])\n",
    "        for token in tokens:\n",
    "            word = token.lemma_\n",
    "            if word in punc:\n",
    "                continue\n",
    "            words.append(word)\n",
    "        \n",
    "    return words\n",
    "\n",
    "\"\"\"\n",
    "word_bags - lista stworzonych z lektur worków słów\n",
    "WAŻNE: w każdym worku pod indeksem 0(liczba, nie string) musi znajdować się autor danego utworu\n",
    "WAŻNE2: używać przed użyciem funkcji count_percents na workach\n",
    "struktura outputu: słownik, gdzie kluczem są autorzy, a wartościami worek słów(z których zostanie wyrzucony klucz 0-z autorem)\n",
    "\"\"\"\n",
    "def combine_word_bags(word_bags):\n",
    "    combined = {}\n",
    "#przechodzenie po wszystkich workach z listy\n",
    "    for bag in word_bags:\n",
    "        keys = combined.keys()\n",
    "#pobieranie autora z aktualnego worka i usuwanie go ze słownika\n",
    "        author = bag[0]\n",
    "        del bag[0]\n",
    "#jeśli autor już jest w wynikowym worku, to aktualizujemy wystąpienia słów u niego\n",
    "        if author in keys:\n",
    "            for elem in bag:\n",
    "#jeśli autor ma już dane słowo, to dodajemy ilość wystąpień z aktualnie iterowanego worka\n",
    "                if elem in combined[author]:\n",
    "                    combined[author][elem] += bag[elem]\n",
    "#jeśli słowa jeszcze nie ma, to dodajemy rekord\n",
    "                else:\n",
    "                    combined[author][elem] = bag[elem]\n",
    "#jeśli autora nie ma jeszcze w wynikowym worku, to dodajemy rekord, przypisując do niego aktualny worek\n",
    "        else:\n",
    "            combined[author] = bag\n",
    "    return combined\n",
    "\n",
    "\"\"\"\n",
    "funkcja zmienia ilosć wystąpień danego słowa na procent wartość wystąpień w tekście\n",
    "\"\"\"\n",
    "def count_percents(word_bag):\n",
    "    amount = 0\n",
    "#zliczanie ilości słów w worku\n",
    "    for elem in word_bag:\n",
    "        if elem == 0:\n",
    "            continue\n",
    "        amount += word_bag[elem]\n",
    "#przeliczanie wystąpień na wartości procentowe\n",
    "    for elem in word_bag:\n",
    "        if elem == 0:\n",
    "            continue\n",
    "        word_bag[elem] = word_bag[elem]/amount*100\n",
    "    return word_bag\n",
    "\n",
    "\"\"\"\n",
    "Funkcja zapisuje worki słów do plików. Folderem jest BagsOfWords/, gdzie pliki tekstowe mają tytuł autor_worka.txt.\n",
    "Format pojedyńczej linii z rekordem: słowo ilość_wystąpień\n",
    "word_bags- słownik worków, gdzie kluczem jest autor, a wartością worek\n",
    "\"\"\"\n",
    "def save_word_bags_to_file(word_bags, directory):\n",
    "    if not os.path.isdir('Bags'):\n",
    "        os.mkdir('Bags')\n",
    "    os.mkdir(directory)\n",
    "    for key in word_bags.keys():\n",
    "        bag = word_bags[key]\n",
    "        file = open(directory + key + \".txt\", \"w\", encoding=\"utf-8\")\n",
    "        for b_keys in bag.keys():\n",
    "            file.write(b_keys + \" \" + str(bag[b_keys]) + '\\n')\n",
    "        file.close()\n",
    "      \n",
    "\"\"\"\n",
    "Funkcja pobiera już wcześniej utworzone worki słów z folderu\n",
    "directory- folder z workami słów \n",
    "WAŻNE:\n",
    "directory podawać bez ukośnika! Czysta nazwa!\n",
    "pliki z workami słów danych autorów muszą mieć za nazwę imię i nazwisko tego autora\n",
    "\"\"\"\n",
    "def get_word_bags_from_directory(directory):\n",
    "    files = os.listdir(directory)\n",
    "    bags = {}\n",
    "    for author in files:\n",
    "        tmp_bag = {}\n",
    "        file = open(directory + \"\\\\\" + author, encoding='utf-8')\n",
    "        for line in file:\n",
    "            line = line.split()\n",
    "            tmp_bag[line[0]] = int(line[1])\n",
    "        bags[repr(author).replace(\".txt\", \"\").replace(\"'\", \"\")] = tmp_bag\n",
    "    return bags\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4521bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To są różne funkcje pomocnicze\n",
    "\"\"\"\n",
    "funkcja pobiera stop wordsy z pliku i zwraca je jako listę\n",
    "\"\"\"\n",
    "def get_stop_words_from_file(file_name):\n",
    "    stops = []\n",
    "    borders = [\"\\\\n\", \"'\"]\n",
    "    with open(file_name, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            single_line = repr(line)\n",
    "            for char in borders:\n",
    "                single_line = single_line.replace(char,\"\")\n",
    "            if not(single_line == \"\"):\n",
    "                stops.append(single_line)\n",
    "    return stops\n",
    "\n",
    "\"\"\"\n",
    "funkcja pobiera autora utworu, przeskakuje tytuł i przeskakuje number ISBN\n",
    "\"\"\"\n",
    "def get_author(file):\n",
    "    author = file.readline()\n",
    "    author = author.replace(\"\\n\",\"\")\n",
    "    author = list(author)\n",
    "    if author[0] == \" \":\n",
    "        author[0] = \"\"\n",
    "    author = \"\".join(author)\n",
    "    \n",
    "    file.readline()\n",
    "    while not (file.readline()  == \"\\n\"):\n",
    "        continue\n",
    "    position_before = file.tell()\n",
    "    if not file.readline().startswith('ISBN'):\n",
    "        file.seek(position_before)\n",
    "    return author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66a6bb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef create_bags_of_words_from_authors(combined=True, with_stop_words=False, lemma=True):\\n    books_list = os.listdir('Books')\\n    word_bags = []\\n    for book in books_list:\\n        word_bags.append(create_bag_of_words_from_file('Books/'+book, with_stop_words, lemma))            \\n    if combined:\\n        word_bags = combine_word_bags(word_bags)            \\n    return word_bags\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Wywołuje sięją po to, aby uzyskać worki słów dla autorów- uprzednio zapisanych lub stworzone na nowo\n",
    "Metoda pomocnicza - automatycznie tworzy worki słów (od parametru combined zależy czy będą to \n",
    "worki słów każdej książki, czy wspólne dla każdego autora)\n",
    "\"\"\"\n",
    "def create_bags_of_words_from_authors(only_stop_words, with_stop_words, lemma):\n",
    "    bags = {}\n",
    "#tworzenie ścieżki folderu na podstawie argumentów\n",
    "    path = \"Bags/BagOfWords_\"\n",
    "    if only_stop_words:\n",
    "        path += \"only\"\n",
    "    elif with_stop_words:\n",
    "        path += \"with\"\n",
    "    elif not with_stop_words:\n",
    "        path += \"no\"\n",
    "    path += \"StopWords_\"\n",
    "    if not lemma:\n",
    "        path += \"Not\"\n",
    "    path += \"Lemma/\"\n",
    "#Jeśli taki folder jużistnieje, oznacza, że jużwcześniej go utworzyliśmy i tylko wczytujemy gotowe worki słów\n",
    "    if os.path.isdir(path):\n",
    "        bags = get_word_bags_from_directory(path)\n",
    "#Jeśli nie ma takiego folderu, to trzeba utworzyć worki\n",
    "    else:\n",
    "        books_list = os.listdir('Books')\n",
    "        word_bags = []\n",
    "        for book in books_list:\n",
    "            print(\"doing \" + book)\n",
    "            word_bags.append(create_bag_of_words_from_file('Books/'+book, False, with_stop_words, lemma, only_stop_words))\n",
    "        bags = combine_word_bags(word_bags)\n",
    "        save_word_bags_to_file(bags, path)\n",
    "    return bags\n",
    "\"\"\"\n",
    "def create_bags_of_words_from_authors(combined=True, with_stop_words=False, lemma=True):\n",
    "    books_list = os.listdir('Books')\n",
    "    word_bags = []\n",
    "    for book in books_list:\n",
    "        word_bags.append(create_bag_of_words_from_file('Books/'+book, with_stop_words, lemma))            \n",
    "    if combined:\n",
    "        word_bags = combine_word_bags(word_bags)            \n",
    "    return word_bags\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba1dc4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tak się tego używa do wczytania worków\n",
    "###########################################\n",
    "#bag = create_bags_of_words_from_authors(False, False, True)\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d17ce075",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Adam Mickiewicz', 'się': 1497, 'w': 1757, 'u': 117, 'o': 348, 'nad': 112, 'i': 2128, 'na': 1304, 'moja': 19, 'ty': 163, 'jak': 579, 'ile': 14, 'trzeba': 38, 'ten': 131, 'tylko': 149, 'kto': 93, 'dziś': 80, 'cały': 133, 'bo': 196, 'po': 323, 'co': 289, 'z': 1482, 'on': 1186, 'ja': 441, 'do': 565, 'gdy': 189, 'od': 244, 'pod': 125, 'twój': 13, 'za': 335, 'tak': 312, 'my': 148, 'tych': 34, 'gdzie': 94, 'a': 565, 'wszystko': 67, 'jakby': 67, 'przed': 104, 'lecz': 206, 'tym': 107, 'że': 555, 'przy': 100, 'nie': 933, 'zapewne': 11, 'dobrze': 51, 'wszystkich': 40, 'właśnie': 38, 'sam': 164, 'jako': 145, 'też': 61, 'mało': 19, 'niż': 26, 'tu': 142, 'był': 136, 'albo': 60, 'daleko': 36, 'obok': 17, 'już': 313, 'nawet': 71, 'by': 79, 'tej': 63, 'to': 531, 'były': 23, 'jeden': 146, 'ale': 176, 'było': 38, 'jeszcze': 101, 'bez': 95, 'była': 48, 'aż': 118, 'znowu': 71, 'nigdy': 41, 'więc': 112, 'przez': 101, 'nim': 29, 'coś': 30, 'być': 275, 'czy': 136, 'według': 1, 'aby': 24, 'pan': 298, 'który': 72, 'teraz': 83, 'ta': 28, 'której': 35, 'tego': 43, 'mój': 70, 'kiedy': 74, 'wiele': 22, 'dla': 118, 'tam': 125, 'jeśli': 73, 'które': 44, 'jakie': 6, 'każdy': 36, 'którym': 21, 'ku': 90, 'jest': 115, 'którego': 19, 'te': 59, 'dlaczego': 6, 'żadna': 3, 'nic': 51, 'inny': 57, 'dwa': 136, 'między': 58, 'żaden': 13, 'są': 31, 'dość': 23, 'czemu': 2, 'wielu': 11, 'żadnych': 1, 'wszyscy': 90, 'gdyby': 48, 'wszystkie': 45, 'kilka': 22, 'których': 33, 'wy': 97, 'moje': 18, 'nasi': 5, 'dużo': 8, 'więcej': 9, 'moim': 13, 'ani': 44, 'taka': 12, 'wszystkim': 12, 'skąd': 10, 'zawsze': 43, 'nasz': 22, 'powinna': 2, 'ach': 28, 'która': 34, 'jaki': 16, 'lub': 63, 'iż': 29, 'takie': 13, 'mimo': 7, 'może': 37, 'nasza': 7, 'będzie': 28, 'jeżeli': 20, 'nasze': 20, 'no': 18, 'bardzo': 62, 'sobie': 60, 'naszego': 5, 'ci': 22, 'pani': 82, 'również': 4, 'został': 5, 'byli': 14, 'sobą': 27, 'żeby': 29, 'można': 18, 'taki': 25, 'dzisiaj': 13, 'którzy': 10, 'niech': 76, 'przecież': 40, 'one': 1, 'także': 27, 'czyli': 20, 'ponad': 5, 'swoje': 15, 'znów': 16, 'podczas': 2, 'twoja': 5, 'oto': 19, 'około': 10, 'sposób': 17, 'jakiś': 4, 'prawie': 18, 'wasz': 17, 'nią': 1, 'twym': 1, 'dlatego': 2, 'jestem': 22, 'naszych': 9, 'będą': 1, 'owszem': 4, 'pomimo': 4, 'tutaj': 1, 'was': 1, 'jakichś': 1, 'dwie': 1, 'natychmiast': 4, 'moi': 6, 'gdzieś': 3, 'ktoś': 6, 'jakaś': 3, 'ich': 1, 'jakoś': 4, 'gdyż': 2, 'wasze': 13, 'im': 3, 'jakiż': 2, 'bynajmniej': 1, 'cokolwiek': 3, 'ależ': 1, 'aj': 2, 'wasza': 1, 'wami': 2, 'kimś': 1, 'twoim': 5, 'twoje': 6, 'żadne': 1, 'przedtem': 1, 'je': 1, 'twoi': 2}\n"
     ]
    }
   ],
   "source": [
    "#create_bag_of_words_from_file(file_name, is_fragment, with_stop_words=False, lemma=True, only_stop_words=False):\n",
    "ab = create_bag_of_words_from_file('panTadeusz.txt', False, False, True, True)\n",
    "print(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40434639",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#string = \"\"\"przykłądowy string w celu testu metody aaaa lisa\n",
    "#nanana ptak leci nad ptakiem i ptak leci z ptakiem obok lisowi lis lis jest zły lisie lis ma bardzo ostre dzik\"\"\"\n",
    "#aaa = create_bag_of_words_from_string(string, {}, True)\n",
    "#print(aaa)\n",
    "#create_bag_of_words_from_file(file_name, is_fragment, with_stop_words=False, lemma=True)\n",
    "#aaa = create_bag_of_words_from_file(\"panTadeusz.txt\", False, True, True)\n",
    "#print(\"\")\n",
    "#print(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c1130ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbooks_list = os.listdir('Books')\\nword_bags = []\\nfor book in books_list:\\n    word_bags.append(create_bag_of_words_from_file('Books/'+book, False, False, False))\\nword_bags = combine_word_bags(word_bags)\\nprint(len(word_bags))\\nfor k in word_bags.keys():\\n    print(k)\\nfor k in word_bags.keys():\\n    word_bags[k] = count_percents(word_bags[k])\\nsave_word_bags_to_file(word_bags)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to jest do moich testów, ale tak to w sumie będzie juzpotem wczytywane\n",
    "\"\"\"\n",
    "books_list = os.listdir('Books')\n",
    "word_bags = []\n",
    "for book in books_list:\n",
    "    word_bags.append(create_bag_of_words_from_file('Books/'+book, False, False, False))\n",
    "word_bags = combine_word_bags(word_bags)\n",
    "print(len(word_bags))\n",
    "for k in word_bags.keys():\n",
    "    print(k)\n",
    "for k in word_bags.keys():\n",
    "    word_bags[k] = count_percents(word_bags[k])\n",
    "save_word_bags_to_file(word_bags)\n",
    "\"\"\"\n",
    "#########################################\n",
    "#Tutaj sytuacja, gdy uruchamiamy po raz kolejny i już mamy gotowe, zapisane worki słów\n",
    "#dicti = get_word_bags_from_directory(\"BagOfWords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aa4398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
