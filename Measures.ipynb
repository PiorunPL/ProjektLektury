{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f5a2d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.WordBag import count_percents\n",
    "from ipynb.fs.full.WordBag import create_bag_of_words_from_file\n",
    "from ipynb.fs.defs.WordBag import combine_word_bags\n",
    "from ipynb.fs.full.WordBag import create_bag_of_words_from_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1982a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af853179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprzykład użycia\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "funkcja liczy miarę fragmentu względem całości\n",
    "jest to suma spierwiastkowanych odległości\n",
    "\"\"\"\n",
    "def calculate_measure(fragment_txt, whole_txt, is_fragment, with_stop_words, lemma, only_stop_words):\n",
    "    fragment = create_bag_of_words_from_file(fragment_txt, is_fragment, with_stop_words, lemma, only_stop_words)\n",
    "    whole = create_bag_of_words_from_file(whole_txt, False)\n",
    "    count_percents(fragment)\n",
    "    count_percents(whole)\n",
    "    measure = 0\n",
    "    for i in (fragment.keys()):\n",
    "        if i in whole:\n",
    "            measure += math.sqrt(abs(fragment[i]-whole[i]))\n",
    "        else:\n",
    "            measure += math.sqrt(fragment[i])\n",
    "    return measure\n",
    "\n",
    "\"\"\"\n",
    "funkcja liczy miarę fragmentu (z pliku) względem połączonych lektur autora\n",
    "jest to suma spierwiastkowanych odległości\n",
    "\"\"\"\n",
    "def calculate_measures(fragment_txt, combined, is_fragment, with_stop_words, lemma, only_stop_words):\n",
    "    fragment = create_bag_of_words_from_file(fragment_txt, is_fragment, with_stop_words, lemma, only_stop_words)\n",
    "    count_percents(fragment)\n",
    "    measures = {}\n",
    "    for author in combined:\n",
    "        count_percents(combined[author])\n",
    "        if author not in measures:\n",
    "            measures[author] = 0\n",
    "        for i in (fragment.keys()):\n",
    "            if i in combined[author]:\n",
    "                measures[author] += math.sqrt(abs(fragment[i]-combined[author][i]))\n",
    "            else:\n",
    "                measures[author] += math.sqrt(fragment[i])\n",
    "    return measures\n",
    "\n",
    "\"\"\"\n",
    "funkcja liczy miarę fragmentu (z napisu) względem połączonych lektur autora\n",
    "jest to suma spierwiastkowanych odległości\n",
    "\"\"\"\n",
    "def calculate_measures_string(fragment_string, combined, with_stop_words, lemma, only_stop_words):\n",
    "    fragment = create_bag_of_words_from_string(fragment_string, {}, with_stop_words, lemma, only_stop_words)\n",
    "    print(fragment)\n",
    "    count_percents(fragment)\n",
    "    measures = {}\n",
    "    for author in combined:\n",
    "        count_percents(combined[author])\n",
    "        if author not in measures:\n",
    "            measures[author] = 0\n",
    "        for i in (fragment.keys()):\n",
    "            if i in combined[author]:\n",
    "                measures[author] += math.sqrt(abs(fragment[i]-combined[author][i]))\n",
    "            else:\n",
    "                measures[author] += math.sqrt(fragment[i])\n",
    "    return measures\n",
    "\n",
    "\"\"\"\n",
    "przykład użycia\n",
    "\"\"\"\n",
    "# calculate_measure('fragment.txt', 'testFile1.txt', True, True, True)\n",
    "\n",
    "# bag1 = create_bag_of_words_from_file('testFile1.txt', False)\n",
    "# bag2 = create_bag_of_words_from_file('testFile2.txt', False)\n",
    "# word_bags = []\n",
    "# word_bags.append(bag1)\n",
    "# word_bags.append(bag2)\n",
    "# combined = combine_word_bags(word_bags)\n",
    "\n",
    "# calculate_measures('fragment.txt', combined, True, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6495d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "funkcja zwraca znormalizowane wyniki (dodawane jest 0,5 w celu poprawienia obserwacji wyników)\n",
    "\"\"\"\n",
    "def normalize_data(measures):\n",
    "    max_value = measures[max(measures, key=measures.get)]\n",
    "    min_value = measures[min(measures, key=measures.get)]\n",
    "    \n",
    "    if min_value == max_value:\n",
    "        return measures\n",
    "    for i in measures:\n",
    "        temp = measures[i]\n",
    "        result = (temp - min_value)/(max_value- min_value) + 0.5\n",
    "        measures[i] = result\n",
    "    \n",
    "    return measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "366c77c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprzykład użycia\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "funkcja zwraca najbardziej prawdopodobnych autorów, czyli takich, przy których miara jest najmniejsza w kolejności rosnącej\n",
    "\"\"\"\n",
    "def find_possible_authors(fragment_txt, combined, is_fragment, with_stop_words, lemma, only_stop_words):\n",
    "    measures = calculate_measures(fragment_txt, combined, is_fragment, with_stop_words, lemma, only_stop_words)\n",
    "    possible_authors = dict(sorted(measures.items(), key=lambda item: item[1]))\n",
    "    return possible_authors\n",
    "\n",
    "\"\"\"\n",
    "funkcja zwraca najbardziej prawdopodobnych autorów z procentową szansą\n",
    "v1 to wersja z dużymi % (nie sumuje sie do 100)\n",
    "v2 to wersja sumująca się do 100\n",
    "v3 to wersja sumująca się do 100, ale pobiera tekst jako string\n",
    "\"\"\"\n",
    "def find_possible_authors_percents_v1(fragment_txt, combined, is_fragment, with_stop_words, lemma, only_stop_words):\n",
    "    sum = 0\n",
    "    measures = calculate_measures(fragment_txt, combined, is_fragment, with_stop_words, lemma, only_stop_words)\n",
    "    measures = normalize_data(measures)\n",
    "    possible_authors_percents = dict(sorted(measures.items(), key=lambda item: item[1]))\n",
    "    for author in possible_authors_percents:\n",
    "        sum += possible_authors_percents[author]\n",
    "    for author in possible_authors_percents:\n",
    "        possible_authors_percents[author]=(sum-possible_authors_percents[author])/sum*100\n",
    "    return possible_authors_percents\n",
    "\n",
    "def find_possible_authors_percents_v2(fragment_txt, combined, is_fragment, with_stop_words, lemma, only_stop_words):\n",
    "    sum = 0\n",
    "    measures = calculate_measures(fragment_txt, combined, is_fragment, with_stop_words, lemma, only_stop_words)\n",
    "    measures = normalize_data(measures)\n",
    "    possible_authors_percents = dict(sorted(measures.items(), key=lambda item: item[1]))\n",
    "    for author in possible_authors_percents:\n",
    "        sum += 1/possible_authors_percents[author]\n",
    "    for author in possible_authors_percents:\n",
    "        possible_authors_percents[author]=1/possible_authors_percents[author]/sum*100\n",
    "    return possible_authors_percents\n",
    "\n",
    "def find_possible_authors_percents_v3(fragment_string, combined, with_stop_words, lemma, only_stop_words):\n",
    "    sum = 0\n",
    "    measures = calculate_measures_string(fragment_string, combined, with_stop_words, lemma, only_stop_words)\n",
    "    measures = normalize_data(measures)\n",
    "    possible_authors_percents = dict(sorted(measures.items(), key=lambda item: item[1]))\n",
    "    for author in possible_authors_percents:\n",
    "        sum += 1/possible_authors_percents[author]\n",
    "    for author in possible_authors_percents:\n",
    "        possible_authors_percents[author]=1/possible_authors_percents[author]/sum*100\n",
    "    return possible_authors_percents\n",
    "\n",
    "\"\"\"\n",
    "przykład użycia\n",
    "\"\"\"\n",
    "# bag1 = create_bag_of_words_from_file('testFile1.txt', False)\n",
    "# bag2 = create_bag_of_words_from_file('testFile2.txt', False)\n",
    "# bag3 = create_bag_of_words_from_file('testFile3.txt', False)\n",
    "# word_bags = []\n",
    "# word_bags.append(bag1)\n",
    "# word_bags.append(bag2)\n",
    "# word_bags.append(bag3)\n",
    "# combined = combine_word_bags(word_bags)\n",
    "\n",
    "# find_possible_authors('fragment.txt', combined, True, True, True)\n",
    "# find_possible_authors_percents_v1('fragment.txt', combined, True, True, True)\n",
    "# find_possible_authors_percents_v2('fragment.txt', combined, True, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f96566a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756419ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ce3677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ff060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd306d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dfd40d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
